# -*- coding: utf-8 -*-
"""Homework5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A7wfgN7bB-TUAWeAyLAPg4J4sKfIv6FL
"""

# GAN Implementation (Q3) - TensorFlow
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# Generator model
def build_generator():
    model = models.Sequential([
        layers.Dense(256, input_dim=100),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(512),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(1024),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(784, activation='tanh'),
        layers.Reshape((28, 28, 1))
    ])
    return model

# Discriminator model
def build_discriminator():
    model = models.Sequential([
        layers.Flatten(input_shape=(28, 28, 1)),
        layers.Dense(512),
        layers.LeakyReLU(0.2),
        layers.Dense(256),
        layers.LeakyReLU(0.2),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# Save sample generated images
def sample_images(generator, epoch):
    noise = np.random.normal(0, 1, (10, 100))
    gen_imgs = generator.predict(noise)
    gen_imgs = 0.5 * gen_imgs + 0.5

    fig, axs = plt.subplots(1, 10, figsize=(20, 2))
    for i in range(10):
        axs[i].imshow(gen_imgs[i, :, :, 0], cmap='gray')
        axs[i].axis('off')
    plt.suptitle(f"Epoch {epoch}")
    plt.savefig(f"gan_mnist_epoch_{epoch}.png")
    plt.close()

# Plot loss curves
def plot_losses(g_losses, d_losses):
    plt.figure(figsize=(10,5))
    plt.plot(g_losses, label="Generator Loss")
    plt.plot(d_losses, label="Discriminator Loss")
    plt.title("GAN Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.savefig("gan_loss_plot.png")
    plt.close()

# Training loop
def train_gan(epochs=100, batch_size=128):
    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    x_train = (x_train.astype(np.float32) - 127.5) / 127.5
    x_train = np.expand_dims(x_train, axis=-1)

    half_batch = batch_size // 2
    generator = build_generator()
    discriminator = build_discriminator()
    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    z = layers.Input(shape=(100,))
    img = generator(z)
    discriminator.trainable = False
    validity = discriminator(img)
    combined = models.Model(z, validity)
    combined.compile(optimizer='adam', loss='binary_crossentropy')

    g_losses, d_losses = [], []

    for epoch in range(epochs + 1):
        idx = np.random.randint(0, x_train.shape[0], half_batch)
        imgs = x_train[idx]

        noise = np.random.normal(0, 1, (half_batch, 100))
        gen_imgs = generator.predict(noise)

        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))
        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))
        d_loss = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])

        noise = np.random.normal(0, 1, (batch_size, 100))
        valid_y = np.ones((batch_size, 1))
        g_loss = combined.train_on_batch(noise, valid_y)

        g_losses.append(g_loss)
        d_losses.append(d_loss)

        if epoch % 50 == 0:
            print(f"Epoch {epoch} / {epochs} | D loss: {d_loss:.4f} | G loss: {g_loss:.4f}")
            sample_images(generator, epoch)

    plot_losses(g_losses, d_losses)

# Run training
if __name__ == '__main__':
    train_gan(epochs=100, batch_size=128)