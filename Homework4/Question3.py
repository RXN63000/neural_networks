# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import numpy as np

def scaled_dot_product_attention(Q, K, V):
    d_k = K.shape[1]

    # Compute dot product of Q and K^T
    scores = np.dot(Q, K.T)

    # Scale by sqrt(d_k)
    scaled_scores = scores / np.sqrt(d_k)

    # Apply softmax to get attention weights
    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=1, keepdims=True)

    # Multiply by V to get output
    output = np.dot(attention_weights, V)

    print("Attention Weights:\n", attention_weights)
    print("Output:\n", output)

# Test the function
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])
scaled_dot_product_attention(Q, K, V)